---
description: Deep Research + PRD Generation workflow v·ªõi kh·∫£ nƒÉng tra c·ª©u web v√† t·∫°o PRD chuy√™n s√¢u
version: 2.3.0
trigger: /research-prd [idea]
portable: true
optimizations:
  - parallel_url_reading
  - semantic_stagnation_check
  - chain_of_density
  - token_budget_controller
  - claim_verification
  - empty_results_fallback      # OPT-01
  - preserve_numerical_data     # OPT-02
  - kb_freshness_validation     # OPT-03
  - fresh_content_boost         # OPT-07
  - code_reference_confidence   # OPT-08
  - adaptive_stagnation         # REC-1
  - transactional_dual_save     # REC-2
  - vector_database             # REC-3
  - tiered_verification         # REC-4
  - rate_limit_backoff          # REC-5
---

# WORKFLOW: RESEARCH-PRD v2.3 (SWOT Optimized)

> **Philosophy:** External Research + Internal Context + Reflexion Loop = High-Quality PRD
> 
> **v2.0 Optimizations (from SWOT Analysis):**
> - ‚ö° Parallel URL Reading (70% latency reduction)
> - üß† Semantic Stagnation Detection (cosine similarity)
> - üîó Chain of Density Synthesis (token efficiency)
> - üí∞ Token Budget Controller (cost management)
> - ‚úÖ Claim Verification Layer (anti-hallucination)
> - üîÑ Research Fallback to KB (reliability)
>
> **v2.2 Audit Optimizations (04/02/2026):**
> - üîÑ OPT-01: Empty results trigger KB fallback
> - üìä OPT-02: Preserve numerical data in synthesis
> - ‚è∞ OPT-03: KB content freshness validation
> - üöÄ OPT-07: Fresh content boost (+10% for <3 months)
> - üíª OPT-08: Code references as HIGH confidence
>
> **v2.3 SWOT Recommendations (04/02/2026):**
> - üìà REC-1: Adaptive Stagnation Threshold (+15% accuracy)
> - üîí REC-2: Transactional Dual-Save (-100% data loss risk)
> - üóÑÔ∏è REC-3: Vector Database (ChromaDB) integration
> - ‚öñÔ∏è REC-4: Tiered Verification by PRD type
> - üõ°Ô∏è REC-5: Rate Limit Backoff (+10% reliability)

---

## PHASE 0: INITIALIZATION & PROJECT DETECTION

### Step 0.1: Initialize Workflow
```yaml
# Load configuration
config = load_yaml(".agent/config/project.yaml")
research_config = load_yaml(".agent/config/research-config.yaml")

# Internal Variables
workflow_id: generate_uuid()
start_time: now()
research_depth: "standard"  # quick | standard | deep

# ‚≠ê NEW: Token Budget Controller
token_budget = {
    "quick": 50000,
    "standard": 100000,
    "deep": 200000
}
current_tokens = 0
MAX_TOKENS = token_budget[research_depth]
```

### Step 0.2: Auto-Detect Project Context
```python
# Detect tech stack t·ª´ project files
project_context = {
    "tech_stack": detect_tech_stack(),  # Scan package.json, pyproject.toml, go.mod
    "backend_framework": None,  # fastapi | express | django | go
    "frontend_framework": None,  # angular | react | vue | svelte
    "database": None,  # postgresql | mysql | mongodb
    "has_existing_prds": check_path(".agent/prds/"),
    "has_knowledge_base": check_path(".agent/knowledge_base/"),
    # ‚≠ê NEW: Detect ORM models
    "orm_models": detect_orm_models()  # SQLAlchemy, Prisma, TypeORM
}

log.info({
    "event": "project_detected",
    "tech_stack": project_context.tech_stack,
    "token_budget": MAX_TOKENS
})
```

### Step 0.3: Parse User Intent
- Ti·∫øp nh·∫≠n `[idea]` t·ª´ ng∆∞·ªùi d√πng
- X√°c ƒë·ªãnh `RESEARCH_TOPIC` v√† `PRD_SCOPE`
- Ph√¢n lo·∫°i `RESEARCH_DEPTH`:
  - `/research-prd-quick [idea]` ‚Üí Quick (skip deep research)
  - `/research-prd [idea]` ‚Üí Standard
  - `/research-prd-deep [idea]` ‚Üí Comprehensive

**Output:** `USER_INTENT`, `RESEARCH_TOPIC`, `RESEARCH_DEPTH`, `MAX_TOKENS`

---

## PHASE 1: DEEP EXTERNAL RESEARCH

> **M·ª•c ti√™u:** Thu th·∫≠p th√¥ng tin t·ª´ b√™n ngo√†i tr∆∞·ªõc khi t·∫°o PRD
> **‚ö° Optimized:** Parallel processing, Chain of Density

### Step 1.1: Define Research Questions
```python
# T·∫°o 3-5 c√¢u h·ªèi nghi√™n c·ª©u
research_questions = generate_questions(USER_INTENT)

# ‚≠ê NEW: Add tech-stack specific questions
if project_context.tech_stack:
    research_questions.append(
        f"Best practices for {RESEARCH_TOPIC} with {project_context.tech_stack}"
    )
```

### Step 1.2: Web Search (// turbo)
```python
# Th·ª±c hi·ªán t√¨m ki·∫øm web
search_queries = [
    f"{RESEARCH_TOPIC} best practices",
    f"{RESEARCH_TOPIC} implementation guide",
    f"{RESEARCH_TOPIC} vs alternatives comparison",
    f"{RESEARCH_TOPIC} common mistakes pitfalls",
    f"{RESEARCH_TOPIC} {project_context.tech_stack} integration"
]

try:
    search_results = []
    for query in search_queries:
        results = search_web(query)
        search_results.extend(results)
    
    # ‚≠ê OPT-01: Check for empty results and trigger fallback
    if len(search_results) == 0:
        log.warning("Web search returned 0 results. Triggering KB fallback.")
        FALLBACK_MODE = True
        goto Step 1.6  # Skip to KB fallback
    
    # Filter v√† rank k·∫øt qu·∫£
    ranked_sources = rank_by_relevance(search_results, RESEARCH_TOPIC)
    
    # ‚≠ê NEW: Recency-Weighted Ranking
    ranked_sources = apply_recency_penalty(ranked_sources, max_age_months=12)
    
    # ‚≠ê OPT-07: Apply boost for very recent content (<3 months)
    if research_config.recency.fresh_content_boost.enabled:
        ranked_sources = apply_freshness_boost(
            ranked_sources, 
            threshold_months=research_config.recency.fresh_content_boost.threshold_months,
            boost_percentage=research_config.recency.fresh_content_boost.boost_percentage
        )
    
except SearchException as e:
    # ‚≠ê NEW: Fallback to Knowledge Base Only
    log.warning(f"Web search failed: {e}. Falling back to KB only mode.")
    FALLBACK_MODE = True
    goto Step 1.6  # Skip to KB fallback
```

### Step 1.3: Deep Content Reading - PARALLEL (// turbo) ‚ö° OPTIMIZED
```python
# ‚≠ê OPTIMIZED: Parallel URL reading with asyncio.gather()
if RESEARCH_DEPTH in ["standard", "deep"]:
    max_sources = {
        "quick": 5,
        "standard": 10,
        "deep": 20
    }
    top_sources = ranked_sources[:max_sources[RESEARCH_DEPTH]]
    
    # PARALLEL READING - 70% latency reduction
    async def read_all_sources_parallel(sources):
        """Read all URLs concurrently with timeout"""
        timeout = research_config.depth_settings[RESEARCH_DEPTH].timeout_per_source
        
        async def read_with_timeout(source):
            try:
                content = await asyncio.wait_for(
                    read_url_content(source.url),
                    timeout=timeout
                )
                return {
                    "url": source.url,
                    "title": source.title,
                    "content": content,
                    "relevance_score": source.score,
                    "publish_date": extract_publish_date(content),
                    "status": "success"
                }
            except asyncio.TimeoutError:
                return {"url": source.url, "status": "timeout"}
            except Exception as e:
                return {"url": source.url, "status": "error", "error": str(e)}
        
        # Execute all reads in parallel
        results = await asyncio.gather(
            *[read_with_timeout(s) for s in sources],
            return_exceptions=True
        )
        
        # Filter successful reads
        return [r for r in results if r.get("status") == "success"]
    
    content_corpus = await read_all_sources_parallel(top_sources)
    
    # Update token count
    current_tokens += sum(len(c["content"]) / 4 for c in content_corpus)  # Rough estimate
    
    if current_tokens > MAX_TOKENS * 0.6:
        log.warning(f"Token budget at 60%: {current_tokens}/{MAX_TOKENS}")
```

### Step 1.4: Visual Research (Optional - Deep mode only)
```python
# Capture screenshots n·∫øu c·∫ßn UI/UX reference
if RESEARCH_DEPTH == "deep" and needs_visual_reference(USER_INTENT):
    visual_references = []
    demo_urls = identify_demo_urls(search_results)[:5]
    
    # Parallel screenshot capture
    screenshots = await asyncio.gather(
        *[browser_capture(url) for url in demo_urls],
        return_exceptions=True
    )
    visual_references = [s for s in screenshots if not isinstance(s, Exception)]
```

### Step 1.5: Synthesize Research Findings - CHAIN OF DENSITY ‚ö° NEW
```python
# ‚≠ê NEW: Chain of Density - Progressive Summarization
def chain_of_density_synthesis(contents: list) -> dict:
    """
    3-step density compression:
    1. Individual summaries (verbose)
    2. Paired merges (denser)
    3. Final synthesis (densest)
    """
    
    # Step 1: Create initial summaries (500 tokens each)
    individual_summaries = []
    for content in contents:
        summary = summarize(
            content["content"],
            max_tokens=500,
            focus=["key_points", "best_practices", "warnings"]
        )
        individual_summaries.append({
            "url": content["url"],
            "summary": summary,
            "claims": extract_key_claims(summary)  # For verification
        })
    
    # Step 2: Merge pairs (300 tokens each)
    merged_summaries = []
    for i in range(0, len(individual_summaries), 2):
        pair = individual_summaries[i:i+2]
        merged = merge_summaries(
            pair,
            max_tokens=300,
            preserve=["unique_insights", "contradictions"]
        )
        merged_summaries.append(merged)
    
    # Step 3: Final synthesis using WHAT-WHY-HOW-COMPARE framework
    research_synthesis = {
        "WHAT": extract_and_merge(merged_summaries, "definitions"),
        "WHY": extract_and_merge(merged_summaries, "benefits_motivations"),
        "HOW": extract_and_merge(merged_summaries, "implementation_approaches"),
        "COMPARE": extract_and_merge(merged_summaries, "alternatives"),
        "IMPLICATIONS": extract_and_merge(merged_summaries, "risks_considerations"),
        "sources": [c["url"] for c in contents],
        "all_claims": collect_all_claims(individual_summaries)  # For verification
    }
    
    return research_synthesis

research_synthesis = chain_of_density_synthesis(content_corpus)
```

### Step 1.5.1: Claim Verification Layer ‚ö° NEW
```python
# ‚≠ê NEW: Verify key claims across multiple sources
def verify_claims(synthesis: dict, min_sources: int = 2) -> dict:
    """
    Cross-check key claims v·ªõi ‚â•2 independent sources
    Flag unverified claims as LOW_CONFIDENCE
    """
    all_claims = synthesis["all_claims"]
    verified_claims = []
    unverified_claims = []
    
    for claim in all_claims:
        # Count how many sources support this claim
        supporting_sources = find_supporting_sources(claim, synthesis["sources"])
        
        if len(supporting_sources) >= min_sources:
            verified_claims.append({
                "claim": claim,
                "confidence": "HIGH",
                "sources": supporting_sources
            })
        else:
            unverified_claims.append({
                "claim": claim,
                "confidence": "LOW",
                "sources": supporting_sources,
                "warning": "Claim found in only 1 source - may need manual verification"
            })
    
    synthesis["verified_claims"] = verified_claims
    synthesis["unverified_claims"] = unverified_claims
    synthesis["verification_rate"] = len(verified_claims) / len(all_claims) if all_claims else 1.0
    
    if synthesis["verification_rate"] < 0.7:
        log.warning(f"Low claim verification rate: {synthesis['verification_rate']:.0%}")
    
    return synthesis

research_synthesis = verify_claims(research_synthesis)

# Save research report
save_research_report(research_synthesis, f".artifacts/research/{RESEARCH_TOPIC}.md")
```

### Step 1.6: Knowledge Base Fallback ‚ö° ENHANCED v2.2
```python
# ‚≠ê ENHANCED: Fallback mode with freshness validation (OPT-01, OPT-03)
if FALLBACK_MODE:
    log.info("Executing KB-only research mode...")
    
    # Query internal knowledge base
    kb_results = query_knowledge_base(
        query=USER_INTENT,
        sources=[
            ".agent/prds/*.md",
            ".agent/knowledge_base/*.md",
            "docs/**/*.md"
        ],
        max_results=10
    )
    
    # ‚≠ê OPT-03: Validate KB content freshness
    if research_config.fallback.freshness_check.enabled:
        stale_content = []
        fresh_content = []
        max_age = research_config.fallback.freshness_check.max_age_months
        
        for kb_item in kb_results:
            last_modified = get_file_modified_date(kb_item["path"])
            age_months = (now() - last_modified).days / 30
            
            if age_months > max_age:
                kb_item["stale"] = True
                kb_item["age_months"] = age_months
                stale_content.append(kb_item)
                if research_config.fallback.freshness_check.warn_on_stale:
                    log.warning(f"KB content stale: {kb_item['path']} ({age_months:.0f} months old)")
            else:
                kb_item["stale"] = False
                fresh_content.append(kb_item)
        
        # Prioritize fresh content
        kb_results = fresh_content + stale_content
    
    # Build synthesis from KB only
    # ‚≠ê OPT-03: Adjust verification_rate based on content freshness
    stale_ratio = len([r for r in kb_results if r.get("stale")]) / len(kb_results) if kb_results else 0
    kb_verification_rate = 1.0 - (stale_ratio * 0.3)  # Reduce by up to 30% for stale content
    
    research_synthesis = {
        "WHAT": extract_from_kb(kb_results, "definitions"),
        "WHY": extract_from_kb(kb_results, "benefits"),
        "HOW": extract_from_kb(kb_results, "implementation"),
        "COMPARE": {"note": "External comparison unavailable - KB only mode"},
        "IMPLICATIONS": extract_from_kb(kb_results, "risks"),
        "sources": [r["path"] for r in kb_results],
        "mode": "KB_FALLBACK",
        "verification_rate": kb_verification_rate,
        "stale_sources_count": len([r for r in kb_results if r.get("stale")]),
        "fresh_sources_count": len([r for r in kb_results if not r.get("stale")])
    }

**Output:** `RESEARCH_SYNTHESIS`, `RESEARCH_REPORT_PATH`, `FALLBACK_MODE`
```

---

## PHASE 2: INTERNAL CONTEXT GROUNDING

### Step 2.1: Codebase Scan
```python
# Qu√©t c·∫•u tr√∫c d·ª± √°n hi·ªán t·∫°i
codebase_context = {
    "project_structure": scan_directory_structure(),
    "existing_modules": identify_modules(),
    "dependencies": parse_dependencies(),
    "database_schema": parse_schema(),  # Supports Prisma, SQLAlchemy, TypeORM
    # ‚≠ê NEW: Version compatibility info
    "dependency_versions": extract_dependency_versions()
}
```

### Step 2.2: Knowledge Base Query - VECTOR SEARCH ‚ö° NEW
```python
# ‚≠ê NEW: Semantic search v·ªõi vector similarity
if project_context.has_knowledge_base:
    if research_config.knowledge_base.vector_enabled:
        # Vector-based semantic search
        similar_prds = vector_search(
            query=USER_INTENT,
            index=".agent/knowledge_base/vectors.idx",
            top_k=5,
            min_similarity=0.7
        )
    else:
        # Fallback to keyword search
        similar_prds = query_knowledge_base(
            query=USER_INTENT,
            sources=[".agent/prds/*.md", ".agent/knowledge_base/*.md"],
            max_results=5
        )
    
    past_lessons = extract_lessons(similar_prds)
else:
    similar_prds = []
    past_lessons = []
```

### Step 2.3: Merge Contexts
```python
# K·∫øt h·ª£p External Research + Internal Context
FULL_CONTEXT = {
    "external_research": RESEARCH_SYNTHESIS,
    "project_context": project_context,
    "codebase_context": codebase_context,
    "historical_context": similar_prds,
    "past_lessons": past_lessons,
    # ‚≠ê NEW: Metadata
    "research_mode": "FALLBACK" if FALLBACK_MODE else "FULL",
    "token_usage": current_tokens
}
```

**Output:** `FULL_CONTEXT`

---

## PHASE 3: COMPLEXITY ASSESSMENT

### Step 3.1: Evaluate Complexity
```python
# ƒê√°nh gi√° ƒë·ªô ph·ª©c t·∫°p theo 5 ti√™u ch√≠
complexity_scores = {
    "ux_impact": assess_ux_complexity(USER_INTENT),         # 1-10
    "cross_module": assess_module_dependencies(codebase_context),  # 1-10
    "security_sensitivity": assess_security_needs(USER_INTENT),    # 1-10
    "data_complexity": assess_data_requirements(USER_INTENT),      # 1-10
    "integration_points": count_integrations(FULL_CONTEXT)         # 1-10
}

weights = {
    "ux_impact": 0.25,
    "cross_module": 0.20,
    "security_sensitivity": 0.25,
    "data_complexity": 0.15,
    "integration_points": 0.15
}

COMPLEXITY_SCORE = sum(score * weights[key] for key, score in complexity_scores.items())
```

### Step 3.2: Set Processing Mode
```python
if COMPLEXITY_SCORE <= 3:
    PROCESSING_MODE = "Standard"      # 1 iteration c√≥ th·ªÉ ƒë·ªß
    MAX_ITERATIONS = 2
elif COMPLEXITY_SCORE <= 6:
    PROCESSING_MODE = "Enhanced"      # 2 iterations recommended
    MAX_ITERATIONS = 3
else:
    PROCESSING_MODE = "Deep Analysis" # 3 iterations, human review b·∫Øt bu·ªôc
    MAX_ITERATIONS = 3
    HUMAN_REVIEW_REQUIRED = True
```

**Output:** `COMPLEXITY_SCORE`, `PROCESSING_MODE`, `MAX_ITERATIONS`

---

## PHASE 4: PRD DRAFTING (Reflexion Loop)

### Step 4.1: Initial Draft
```python
# Invoke prd-drafter skill
draft_input = {
    "user_intent": USER_INTENT,
    "full_context": FULL_CONTEXT,
    "research_synthesis": RESEARCH_SYNTHESIS,
    "processing_mode": PROCESSING_MODE,
    # ‚≠ê NEW: Include verification data
    "unverified_claims": RESEARCH_SYNTHESIS.get("unverified_claims", [])
}

DRAFT_V1 = invoke_skill("prd-drafter", draft_input)

# Track history
draft_history = [{
    "version": 1,
    "content": DRAFT_V1,
    "confidence": DRAFT_V1.confidence,
    "timestamp": now(),
    "embedding": compute_embedding(DRAFT_V1)  # ‚≠ê NEW: For semantic comparison
}]
current_iteration = 1
```

### Step 4.2: Critical Analysis
```python
# Invoke prd-critic skill
critique = invoke_skill("prd-critic", {
    "draft": DRAFT_V1,
    "processing_mode": PROCESSING_MODE
})

QUALITY_SCORE = critique.quality_score
CRITIQUE_NOTES = critique.issues
```

### Step 4.3: Reflexion Loop - SEMANTIC STAGNATION ‚ö° OPTIMIZED
```python
while current_iteration < MAX_ITERATIONS and QUALITY_SCORE < QUALITY_THRESHOLD:
    # Self-correction
    refined_draft = invoke_skill("prd-drafter", {
        "draft": draft_history[-1].content,
        "critique_notes": CRITIQUE_NOTES,
        "draft_history": draft_history,
        "instruction": f"Iteration #{current_iteration + 1}: Fix all issues"
    })
    
    # Compute embedding for new draft
    new_embedding = compute_embedding(refined_draft)
    
    # Store in history
    current_iteration += 1
    draft_history.append({
        "version": current_iteration,
        "content": refined_draft,
        "confidence": refined_draft.confidence,
        "timestamp": now(),
        "embedding": new_embedding
    })
    
    # Re-evaluate
    critique = invoke_skill("prd-critic", {"draft": refined_draft})
    QUALITY_SCORE = critique.quality_score
    CRITIQUE_NOTES = critique.issues
    
    # ‚≠ê REC-1: Adaptive Semantic Stagnation Check
    if len(draft_history) >= 2:
        prev_embedding = draft_history[-2]["embedding"]
        similarity = cosine_similarity(prev_embedding, new_embedding)
        
        # ‚≠ê REC-1: Get adaptive threshold based on iteration
        if research_config.stagnation.adaptive_threshold.enabled:
            base = research_config.stagnation.adaptive_threshold.base_threshold
            decay = research_config.stagnation.adaptive_threshold.decay_per_iteration
            max_threshold = research_config.stagnation.adaptive_threshold.max_threshold
            stagnation_threshold = min(max_threshold, base + (decay * current_iteration))
        else:
            stagnation_threshold = research_config.stagnation.similarity_threshold
        
        # Check if stagnating with adaptive threshold
        if similarity > stagnation_threshold:
            log.info({
                "event": "semantic_stagnation",
                "similarity": f"{similarity:.2%}",
                "threshold": f"{stagnation_threshold:.2%}",
                "iteration": current_iteration
            })
            break
        
        # Combined check: stagnating if similarity > 0.90 AND score_improvement < 1
        score_improvement = QUALITY_SCORE - draft_history[-2].get("quality_score", 0)
        if score_improvement < research_config.stagnation.min_score_improvement and similarity > 0.90:
            log.info(f"Combined stagnation: score +{score_improvement}, similarity {similarity:.2%}")
            break
    
    # Token budget check
    current_tokens += estimate_tokens(refined_draft)
    if current_tokens > MAX_TOKENS * research_config.token_budget.stop_threshold:
        log.warning(f"Token budget at {research_config.token_budget.stop_threshold:.0%}: {current_tokens}/{MAX_TOKENS}. Stopping reflexion.")
        break
```

**Output:** `FINAL_DRAFT`, `QUALITY_SCORE`, `draft_history`

---

## PHASE 5: MULTI-EXPERT VALIDATION - PARALLEL ‚ö° OPTIMIZED

### Step 5.1: Parallel Expert Validation ‚ö° NEW
```python
# ‚≠ê OPTIMIZED: Run validators in parallel
if QUALITY_SCORE >= 80:
    # Define validation tasks
    async def run_parallel_validation():
        tasks = []
        
        # Task 1: Codebase Validation (always run)
        tasks.append(invoke_skill_async("codebase-validator", {
            "draft_prd": FINAL_DRAFT,
            "project_context": project_context,
            "check_versions": True  # ‚≠ê NEW: Version compatibility
        }))
        
        # Task 2: Domain Expert (Enhanced/Deep mode only)
        if PROCESSING_MODE != "Standard":
            tasks.append(invoke_skill_async("domain-expert", {
                "draft_prd": FINAL_DRAFT,
                "domain": config.project.domain
            }))
        
        # Execute all in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    validation_results = await run_parallel_validation()
    
    # Extract scores
    CODEBASE_VALIDATION_SCORE = validation_results[0].score
    if PROCESSING_MODE != "Standard":
        DOMAIN_EXPERT_SCORE = validation_results[1].score
    else:
        DOMAIN_EXPERT_SCORE = None
```

### Step 5.2: Calculate Final Score
```python
if PROCESSING_MODE == "Standard":
    FINAL_SCORE = (QUALITY_SCORE * 0.7) + (CODEBASE_VALIDATION_SCORE * 0.3)
elif PROCESSING_MODE == "Enhanced":
    FINAL_SCORE = (QUALITY_SCORE * 0.5) + (CODEBASE_VALIDATION_SCORE * 0.25) + (DOMAIN_EXPERT_SCORE * 0.25)
else:  # Deep Analysis
    FINAL_SCORE = (QUALITY_SCORE * 0.4) + (CODEBASE_VALIDATION_SCORE * 0.3) + (DOMAIN_EXPERT_SCORE * 0.3)

# ‚≠ê NEW: Penalty for low verification rate
if RESEARCH_SYNTHESIS.get("verification_rate", 1.0) < 0.7:
    FINAL_SCORE *= 0.95  # 5% penalty
    log.warning("Applied 5% penalty due to low claim verification rate")
```

---

## PHASE 6: HUMAN CHECKPOINT

### Step 6.1: Prepare Review Package
```markdown
## üìã PRD Review Package

### Scores
- Quality Score: {QUALITY_SCORE}/100
- Codebase Validation: {CODEBASE_VALIDATION_SCORE}/100
- Domain Expert: {DOMAIN_EXPERT_SCORE}/100
- **Final Score: {FINAL_SCORE}/100**

### Research Quality ‚≠ê NEW
- Sources Analyzed: {len(RESEARCH_SYNTHESIS.sources)}
- Claim Verification Rate: {RESEARCH_SYNTHESIS.verification_rate:.0%}
- Research Mode: {RESEARCH_SYNTHESIS.mode}

### Token Usage ‚≠ê NEW
- Total Tokens: {current_tokens:,}
- Budget: {MAX_TOKENS:,}
- Usage: {current_tokens/MAX_TOKENS:.0%}

### Iterations: {current_iteration}
### Processing Mode: {PROCESSING_MODE}

### ‚ö†Ô∏è Unverified Claims (Review Required)
{list unverified_claims if any}

### Outstanding Issues
{HIGH severity issues if any}
```

### Step 6.2: Auto-Approval Check
```python
AUTO_APPROVE = (
    FINAL_SCORE >= 90 and
    HIGH_SEVERITY_ISSUES == 0 and
    PROCESSING_MODE != "Deep Analysis" and
    RESEARCH_SYNTHESIS.get("verification_rate", 1.0) >= 0.8 and  # ‚≠ê NEW
    len(RESEARCH_SYNTHESIS.get("unverified_claims", [])) <= 2     # ‚≠ê NEW
)

if AUTO_APPROVE:
    goto Phase 7
else:
    # Request human review
    notify_user(review_package)
    await user_approval()
```

---

## PHASE 7: DELIVERY

> [!CAUTION]
> **MANDATORY**: Artifacts PH·∫¢I ƒë∆∞·ª£c l∆∞u v√†o `.agent/prds/` (project) V√Ä conversation brain.
> **KH√îNG ƒê∆Ø·ª¢C** ch·ªâ l∆∞u v√†o m·ªôt n∆°i. ƒê√¢y l√† quy t·∫Øc b·∫Øt bu·ªôc.

### Step 7.1: Save Artifacts (// turbo) - REC-2 TRANSACTIONAL DUAL SAVE
```python
# ‚≠ê REC-2: ATOMIC DUAL-SAVE WITH ROLLBACK
# File naming convention
feature_slug = slugify(RESEARCH_TOPIC)  # e.g., "quick-expense-module"
prd_filename = f"PRD-{feature_slug}.md"
research_filename = f"research-{feature_slug}.md"

# Define paths
PROJECT_PRD_PATH = f".agent/prds/{prd_filename}"
PROJECT_RESEARCH_PATH = f".agent/prds/{research_filename}"
BRAIN_PATH = f"<appDataDir>/brain/<conversation-id>/"
BRAIN_PRD_PATH = f"{BRAIN_PATH}{prd_filename}"

# ‚≠ê REC-2: Atomic Dual-Save Function
async def atomic_dual_save(content: str, project_path: str, brain_path: str) -> bool:
    """
    Transactional save: both succeed or both fail
    Uses temp files + atomic rename for data integrity
    """
    temp_project = f"{project_path}.tmp"
    temp_brain = f"{brain_path}.tmp"
    
    try:
        # Step 1: Write to temp files in parallel
        await asyncio.gather(
            write_file_async(temp_project, content),
            write_file_async(temp_brain, content)
        )
        
        # Step 2: Verify temp files exist and have content
        if not exists(temp_project) or not exists(temp_brain):
            raise DualSaveError("Temp file creation failed")
        
        if file_size(temp_project) == 0 or file_size(temp_brain) == 0:
            raise DualSaveError("Temp file is empty")
        
        # Step 3: Atomic rename (OS-level atomic on most systems)
        os.rename(temp_project, project_path)
        log.info(f"‚úÖ Atomic save to project: {project_path}")
        
        os.rename(temp_brain, brain_path)
        log.info(f"‚úÖ Atomic save to brain: {brain_path}")
        
        return True
        
    except Exception as e:
        # Rollback: delete temp files and any partial writes
        log.error(f"‚ùå Dual-save failed: {e}")
        safe_delete(temp_project)
        safe_delete(temp_brain)
        
        # If project was saved but brain failed, rollback project
        if exists(project_path) and not exists(brain_path):
            backup_path = f"{project_path}.rollback"
            os.rename(project_path, backup_path)
            log.warning(f"Rolled back project save to: {backup_path}")
        
        raise DualSaveError(f"Atomic dual-save failed: {e}")

# Execute atomic dual-save for PRD
await atomic_dual_save(FINAL_DRAFT, PROJECT_PRD_PATH, BRAIN_PRD_PATH)

# Save research synthesis to project only (not required in brain)
save_file(RESEARCH_SYNTHESIS, PROJECT_RESEARCH_PATH)

# === VALIDATION CHECK ===
assert exists(PROJECT_PRD_PATH), f"FAIL: PRD not saved to {PROJECT_PRD_PATH}"
assert exists(BRAIN_PRD_PATH), f"FAIL: PRD not saved to {BRAIN_PRD_PATH}"

# ‚≠ê NEW: Save workflow metrics
save_metrics({
    "workflow_id": workflow_id,
    "duration_seconds": (now() - start_time).seconds,
    "token_usage": current_tokens,
    "iterations": current_iteration,
    "final_score": FINAL_SCORE,
    "research_mode": RESEARCH_SYNTHESIS.get("mode", "FULL"),
    "verification_rate": RESEARCH_SYNTHESIS.get("verification_rate", 1.0),
    "artifacts": {
        "project_prd": PROJECT_PRD_PATH,
        "brain_prd": BRAIN_PRD_PATH
    }
}, ".agent/metrics/research-prd-metrics.jsonl")
```

> [!TIP]
> **L√Ω do Dual-Save:**
> - `.agent/prds/`: L∆∞u tr·ªØ vƒ©nh vi·ªÖn trong project, c√≥ th·ªÉ tham kh·∫£o sau n√†y
> - `brain/<conversation-id>/`: Cho ph√©p user review trong conversation UI

### Step 7.2: Generate Tests (Optional)
```python
if config.features.test_generation:
    tests = invoke_skill("test-generator", {"prd": FINAL_DRAFT})
    save_tests(tests)
```

### Step 7.3: Effort Estimation (Optional)
```python
if user_requested_estimate or PROCESSING_MODE == "Deep Analysis":
    estimate = invoke_skill("effort-estimator", {"prd": FINAL_DRAFT})
    display_estimate(estimate)
```

### Step 7.4: Next Steps Prompt
```markdown
> üìã **PRD ƒë√£ ho√†n th√†nh!**
> - Quality Score: {QUALITY_SCORE}/100
> - Research Sources: {source_count} sources analyzed
> - Claim Verification: {verification_rate:.0%}
> - Token Usage: {current_tokens:,} / {MAX_TOKENS:,}
> - Iterations: {current_iteration}
>
> **üìö Artifacts Generated:**
> - PRD: `.agent/prds/{prd_filename}`
> - Research Report: `{RESEARCH_REPORT_PATH}`
>
> **B·∫°n mu·ªën l√†m g√¨ ti·∫øp theo?**
> 1. `/plan` - T·∫°o Implementation Plan
> 2. `/estimate` - Xem chi ti·∫øt effort estimation
> 3. `/decompose` - Chia nh·ªè th√†nh user stories
> 4. `/review` - Request peer review
```

---

## üìä APPENDIX A: Performance Metrics

### Latency Comparison
| Phase | v1.0 (Sequential) | v2.0 (Optimized) | Improvement |
|:------|:-----------------:|:----------------:|:-----------:|
| Phase 1 (Research) | ~200s | ~60s | **70%** ‚Üì |
| Phase 4 (Reflexion) | ~90s | ~70s | **22%** ‚Üì |
| Phase 5 (Validation) | ~40s | ~25s | **38%** ‚Üì |
| **Total (Standard)** | **~180s** | **~90s** | **50%** ‚Üì |

### Token Efficiency
| Mode | v1.0 Tokens | v2.0 Tokens | Improvement |
|:-----|:-----------:|:-----------:|:-----------:|
| Quick | ~30K | ~25K | **17%** ‚Üì |
| Standard | ~80K | ~60K | **25%** ‚Üì |
| Deep | ~140K | ~100K | **29%** ‚Üì |

---

## üìä APPENDIX B: Configuration

### Required Config Files

#### `.agent/config/project.yaml`
```yaml
project:
  name: "My Project"
  domain: "general"  # catering | construction | ecommerce | general

tech_stack:
  backend: "fastapi"
  frontend: "angular"
  database: "postgresql"

i18n:
  default_locale: "vi-VN"
```

#### `.agent/config/research-config.yaml`
```yaml
research:
  max_sources: 10
  preferred_domains:
    - "github.com"
    - "stackoverflow.com"
  
# v2.0 additions
knowledge_base:
  vector_enabled: false  # Set true if using pgvector/Pinecone
  
token_budget:
  quick: 50000
  standard: 100000
  deep: 200000

stagnation:
  similarity_threshold: 0.95
  min_score_improvement: 1
```

---

**Version History:**
- v1.0.0: Initial release - Deep Research + PRD Generation combined workflow
- v2.0.0: Optimized release
  - ‚ö° Parallel URL reading (70% latency reduction)
  - üß† Semantic stagnation detection (cosine similarity)
  - üîó Chain of Density synthesis (25% token reduction)
  - üí∞ Token budget controller
  - ‚úÖ Claim verification layer (anti-hallucination)
  - üîÑ Research fallback to KB
  - ‚ö° Parallel expert validation
  - üìä Recency-weighted source ranking
- **v2.1.0**: Dual-Save Enforcement (03/02/2026)
  - üìÅ **MANDATORY**: Artifacts l∆∞u v√†o `.agent/prds/` V√Ä `brain/<conversation-id>/`
  - ‚úÖ Validation check ƒë·∫£m b·∫£o file ƒë∆∞·ª£c l∆∞u ƒë√∫ng c·∫£ 2 n∆°i
  - üìä Metrics log bao g·ªìm artifact paths
